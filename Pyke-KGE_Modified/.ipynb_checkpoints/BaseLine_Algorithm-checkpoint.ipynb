{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from collections import ChainMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load the test data\n",
    "test_dataset = pickle.load( open( \"testing_data.p\", \"rb\" ) )\n",
    "\n",
    "type(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "## In this cell we segrigate inputs and labels for testing the model\n",
    "x_test, y_test = list(), list()\n",
    "nonecount1 = list()\n",
    "#print('First padding',a[0])\n",
    "for k,i in enumerate(test_dataset):\n",
    "    if isinstance(i,list):\n",
    "        \n",
    "        seq_x_test, seq_y_test = i[:len(i) - 1], i[-1]\n",
    "        x_test.append(seq_x_test)\n",
    "        y_test.append(seq_y_test)\n",
    "    else:\n",
    "        nonecount.append(k)\n",
    "print(len(nonecount1))\n",
    "\n",
    "# convert the list to numpy array\n",
    "num_x_test = np.array(x_test)\n",
    "num_y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell calculates the centroid for each x's and store it in tuples along with x's\n",
    "for i in num_x_test:\n",
    "    centroid = np.mean(i, axis=0)\n",
    "    new_tuple = (i,centroid)       ## remember the order in the tuple for later use\n",
    "    with open('all_centroids.p', 'ab') as f:\n",
    "        pickle.dump(new_tuple, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell captures centroids along with x's(inputs) in a tuple\n",
    "with open('all_centroids.p', 'rb') as f: \n",
    "    centroid_list = [pickle.load(f) for j in range(len(num_x_test))] \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for testing\n",
    "sample_centroid_list = centroid_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary with only the item embeddings\n",
    "\n",
    "item_embedddings = pickle.load( open( \"item_embedddings.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this cell stores the normalized item_embeddings\n",
    "nomalized_item_embedddings = {}\n",
    "\n",
    "for k,v in item_embedddings.items():\n",
    "    v_norm = np.linalg.norm(v)\n",
    "    nomalized_item_embedddings.update({k:v_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:22,  2.48s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:18,  2.32s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:06<00:15,  2.22s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:08<00:12,  2.14s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:10<00:10,  2.10s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:12<00:08,  2.06s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:14<00:06,  2.04s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:16<00:04,  2.03s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:18<00:02,  2.02s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:20<00:00,  2.03s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## this cell calculates the cosine similarity among y and y_hats\n",
    "final_cosine = []\n",
    "ranks = []\n",
    "\n",
    "\n",
    "for tuples in tqdm(centroid_list):  \n",
    "    local_cosine = {}\n",
    "    sorted_cosine = {}\n",
    "    rank_list = []\n",
    "    sliced_dict = {}\n",
    "    inputs,centroid = tuples\n",
    "    centroid_norm = np.linalg.norm(centroid)\n",
    "    original_item_embedding_values = list(item_embedddings.values())\n",
    "    count = 0\n",
    "    for k,v in nomalized_item_embedddings.items():\n",
    "        if v not in inputs:\n",
    "            \n",
    "            # manually compute cosine similarity\n",
    "            dot = np.dot(centroid, original_item_embedding_values[count])\n",
    "            count +=1\n",
    "            result = dot / (centroid_norm * v)\n",
    "            local_cosine.update({k:result})\n",
    "   \n",
    "    sorted_cosine = {k: v for k, v in sorted(local_cosine.items(), key=lambda item: -item[1])}\n",
    "    sliced_dict = dict(itertools.islice(sorted_cosine.items(), 60))  ## take top 50 prediction for each item\n",
    "    \n",
    "    rank_list = list(sliced_dict.keys())\n",
    "    with open('baseline_ranks_index.p', 'ab') as f:\n",
    "        pickle.dump(rank_list, f)\n",
    "    del original_item_embedding_values,inputs,centroid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell captures the top n ranked predicted items\n",
    "with open('baseline_ranks_index.p', 'rb') as f: \n",
    "    y_hat = [pickle.load(f) for j in range(len(centroid_list))]  ## need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b34efb234cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_output_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem_embedddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## load original keys of y\n",
    "y = []\n",
    "original_output_embeddings = num_y_test.tolist()\n",
    "for i in original_output_embeddings:\n",
    "    for k,v in item_embedddings.items():\n",
    "        if i ==v:\n",
    "            y.append(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is to generate the matrix for final learning\n",
    "## so if original label is present in top n prediction labels , increase the count by 1\n",
    "y_arr = [None]* len(y)\n",
    "y_hat_arr = [None]* len(y_hat)\n",
    "y_arr = y\n",
    "y_hat_arr = y_hat\n",
    "\n",
    "i = 0\n",
    "count = 0\n",
    "while(i<len(y_arr)):\n",
    "    if (y_arr[i] in y_hat_arr[i]):\n",
    "        count +=1\n",
    "    i +=1\n",
    "Ratio = count/len(y_arr)\n",
    "print('Learning Ratio is : ',Ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
