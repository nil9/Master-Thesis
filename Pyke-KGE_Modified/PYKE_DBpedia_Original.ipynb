{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_classes import PYKE\n",
    "from helper_classes import Parser\n",
    "from helper_classes import DataAnalyser\n",
    "from helper_classes import PPMI\n",
    "import itertools\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import util as ut\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DEFINE MODEL PARAMS\n",
    "K = 45\n",
    "num_of_dims = 50\n",
    "bound_on_iter = 30\n",
    "omega = 0.45557\n",
    "e_release = 0.0414\n",
    "final_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kg_root = '/media/sami/70AE0846AE080776/folder2'\n",
    "kg_path = kg_root + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Preprocessing  starts ######\n",
      "\n",
      "\n",
      "###### Constructing Inverted Index  starts ######\n",
      "Number of RDF triples: 22367362\n",
      "Number of vocabulary terms:  303239\n",
      "Number of subjects:  152063\n",
      "Constructing Inverted Index  took  1058.7494292259216  seconds\n",
      "\n",
      "\n",
      "\n",
      "###### Calculation of PPMIs  starts ######\n",
      "Calculation of PPMIs  took  97.17543578147888  seconds\n",
      "\n",
      "Preprocessing  took  1157.0186760425568  seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "storage_path, experiment_folder = ut.create_experiment_folder()\n",
    "\n",
    "parser = Parser(p_folder=storage_path, k=K)\n",
    "\n",
    "parser.set_similarity_measure(PPMI)\n",
    "\n",
    "model = PYKE()\n",
    "\n",
    "analyser = DataAnalyser(p_folder=storage_path)\n",
    "# For the illustration purpusoes lets only process first 5000 ntriples from each given file.\n",
    "# To reproduce  reported results => parser.pipeline_of_preprocessing(kg_path)\n",
    "holder = parser.pipeline_of_preprocessing(kg_path,bound=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Generating Embeddings:  starts ######\n",
      "EPOCH:  0\n",
      "EPOCH:  1\n",
      "EPOCH:  2\n",
      "EPOCH:  3\n",
      "EPOCH:  4\n",
      "EPOCH:  5\n",
      "EPOCH:  6\n",
      "EPOCH:  7\n",
      "EPOCH:  8\n",
      "EPOCH:  9\n",
      "EPOCH:  10\n",
      "EPOCH:  11\n",
      "EPOCH:  12\n",
      "EPOCH:  13\n",
      "EPOCH:  14\n",
      "EPOCH:  15\n",
      "EPOCH:  16\n",
      "EPOCH:  17\n",
      "EPOCH:  18\n",
      "EPOCH:  19\n",
      "EPOCH:  20\n",
      "EPOCH:  21\n",
      "EPOCH:  22\n",
      "EPOCH:  23\n",
      "EPOCH:  24\n",
      "\n",
      " Epoch:  24\n",
      "System energy: -0.03499999999999983\n",
      "Generating Embeddings:  took  82035.81564879417  seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(holder)\n",
    "key_list = list(holder.keys())\n",
    "embeddings = ut.randomly_initialize_embedding_space(vocab_size, num_of_dims)\n",
    "\n",
    "#for i in range(len(key_list)):\n",
    "#   embeddings[key_list[i]] = embeddings_values[i]1\n",
    "\n",
    "learned_embeddings = model.pipeline_of_learning_embeddings(e=embeddings,\n",
    "                                                           max_iteration=bound_on_iter,\n",
    "                                                           energy_release_at_epoch=e_release,\n",
    "                                                           holder=holder, omega=omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = pd.Series(key_list)\n",
    "learned_embeddings.insert(loc=0, column='new_column', value=column_values)\n",
    "final_embeddings = learned_embeddings.set_index('new_column').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame.from_dict(data=final_embeddings, orient='index')\n",
    "   .to_csv(storage_path +'/PYKE_50_embd.csv', header=True))\n",
    "\n",
    "# To use memory efficiently\n",
    "del holder\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_info = ut.deserializer(path=storage_path, serialized_name='type_info')\n",
    "len(type_info)# denoted as \\mathcal{S} in the paper\n",
    "\n",
    "# get the index of objects / get type information =>>> s #type o\n",
    "all_types = sorted(set.union(*list(type_info.values())))\n",
    "len(all_types)# denoted as C in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://hm2.com/class#Article\n",
      "http://hm2.com/class#Colour\n",
      "http://hm2.com/class#Size\n",
      "http://hm2.com/class#HM\n"
     ]
    }
   ],
   "source": [
    "vocabulary = ut.deserializer(path=storage_path, serialized_name='vocabulary')\n",
    "for i in all_types:\n",
    "    print(vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell extracts the url from url list which contanins the embeddings for an item\n",
    "\n",
    "\n",
    "path_root = '/media/sami/70AE0846AE080776/folder2'\n",
    "\n",
    "\n",
    "all_urls = []\n",
    "for root, dirs, files in os.walk(path_root):\n",
    "    for file in files:\n",
    "        links = []\n",
    "        to_Read = os.path.join(root, file)\n",
    "        f = open(to_Read,\"r\")\n",
    "        urls = f.readline()\n",
    "        while(urls):\n",
    "            links.append(re.findall('^<http:\\/\\/hm2\\.com\\/article#\\d{1,18}>', urls)) \n",
    "            urls = f.readline()\n",
    "        # convert list of list to list\n",
    "        merged = list(itertools.chain.from_iterable(links))\n",
    "        # remove duplicates\n",
    "        unique_urls = list(set(merged))\n",
    "        # remove special charecter < ,>\n",
    "        removetable = str.maketrans('', '', '<>')\n",
    "        rmv_specialchar = [s.translate(removetable) for s in unique_urls]\n",
    "        f.close()\n",
    "        # store in final list\n",
    "        all_urls.append(rmv_specialchar)\n",
    "        # empty the lists\n",
    "        del links[:]\n",
    "        del merged\n",
    "        del unique_urls\n",
    "        del rmv_specialchar\n",
    "\n",
    "#print(all_urls)\n",
    "pickle.dump( all_urls, open(storage_path+\"/all_urllist.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n"
     ]
    }
   ],
   "source": [
    "# checks the maximum no of products from the transaction list\n",
    "#define the function#\n",
    "def find_max_list(list):\n",
    "    list_len = [len(i) for i in list]\n",
    "    print(max(list_len))\n",
    "\n",
    "#print output#\n",
    "find_max_list(all_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction: 718484\n",
      "Frequency of items: Counter({2: 276092, 3: 163718, 4: 97769, 5: 59306, 6: 36823, 7: 23303, 8: 15520, 9: 10307, 1: 8709, 10: 7151, 11: 4908, 12: 3683, 13: 2616, 14: 2008, 15: 1421, 16: 1079, 17: 764, 18: 635, 19: 500, 20: 397, 21: 301, 22: 218, 23: 195, 24: 142, 25: 125, 26: 105, 27: 98, 28: 79, 29: 54, 32: 42, 31: 41, 30: 40, 33: 29, 34: 27, 35: 21, 36: 20, 39: 17, 37: 14, 42: 14, 38: 13, 41: 10, 44: 10, 40: 9, 57: 8, 46: 8, 52: 7, 51: 6, 50: 6, 68: 5, 47: 5, 56: 5, 45: 4, 48: 4, 53: 4, 69: 3, 60: 3, 59: 3, 82: 3, 58: 3, 43: 3, 86: 3, 65: 3, 88: 3, 49: 3, 78: 3, 72: 2, 70: 2, 111: 2, 66: 2, 74: 2, 62: 2, 80: 2, 96: 2, 79: 2, 98: 1, 54: 1, 165: 1, 87: 1, 115: 1, 214: 1, 221: 1, 118: 1, 116: 1, 149: 1, 123: 1, 180: 1, 182: 1, 206: 1, 130: 1, 202: 1, 107: 1, 97: 1, 114: 1, 55: 1, 208: 1, 146: 1, 67: 1, 93: 1, 83: 1, 75: 1, 266: 1, 85: 1, 140: 1, 169: 1, 109: 1, 71: 1, 64: 1, 105: 1, 91: 1, 77: 1, 102: 1, 61: 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "# this cell provides the list of producs for all the transactions\n",
    "counts = []\n",
    "# Iterate over the list and to count internal elements in lists \n",
    "count = 0\n",
    "for listElem in all_urls:\n",
    "    counts.append(len(listElem))                    \n",
    "#print('Number of elements count : ', counts)\n",
    "print('Number of transaction:',len(counts))\n",
    "\n",
    "counter=collections.Counter(counts)\n",
    "print('Frequency of items:',counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store transactions upto 10 products long\n",
    "\n",
    "\n",
    "unusual_length=[]\n",
    "intended_urls = [i for i in all_urls if len(i)< 11 and len(i)>1] # transaction length = 10, that is why <11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell extracts the embeddings of the corrosponding items\n",
    "\n",
    "\n",
    "final_list = []\n",
    "\n",
    "\n",
    "for items in intended_urls:\n",
    "    \n",
    "    new_list = []\n",
    "    for key in items:\n",
    "        \n",
    "        \n",
    "        if key in final_embeddings:\n",
    "           \n",
    "            new_list.append(final_embeddings[key])\n",
    "    final_list.append(new_list)\n",
    "            \n",
    "pickle.dump( final_list, open(storage_path+\"/extracted_embedddings.p\", \"wb\" ) )            \n",
    "\n",
    "#print(final_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell does the left padding for each transaction \n",
    "\n",
    "a = [None] * len(final_list)\n",
    "\n",
    "# hardcoded the the maximum no of product in the trabsaction list as 5 need to change \n",
    "for i in range(0,len(final_list)):\n",
    "    if len(final_list[i])<11:\n",
    "        \n",
    "        \n",
    "        pad = 10 - len(final_list[i])\n",
    "        a[i] = [[0]*50]*pad+(final_list[i])\n",
    "    \n",
    "#print(a)\n",
    "pickle.dump( a, open(storage_path+\"/padded_embedding.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'key_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-458393d8c44d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http:\\/\\/hm2\\.com\\/article#\\d{1,18}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnewlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Read Note\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The unique items from the entire data set:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'key_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Number of unique items in the entire data set, I have excluded transactions greter than 10 products but in below \n",
    "# cell the unique items from the entire data set is counted\n",
    "\n",
    "r = re.compile(\"http:\\/\\/hm2\\.com\\/article#\\d{1,18}\")\n",
    "newlist = list(filter(r.match, key_list)) # Read Note\n",
    "print('The unique items from the entire data set:',len(newlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# this cell prepares the input data and output labels for final LSTM, here the x's are the input and y's are \n",
    "#the labels\n",
    "x, y = list(), list()\n",
    "nonecount = list()\n",
    "#print('First padding',a[0])\n",
    "for k,i in enumerate(a):\n",
    "    if isinstance(i,list):\n",
    "        \n",
    "        seq_x, seq_y = i[:len(i) - 1], i[-1]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    else:\n",
    "        nonecount.append(k)\n",
    "print(len(nonecount))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing final embedding dictionary\n",
    "\n",
    "pickle.dump( final_embeddings, open(storage_path+\"/Pyke50_pickle.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
