{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# univariate lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import spatial\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padded_embedding = pickle.load( open( \"padded_embedding.p\", \"rb\" ) )\n",
    "#print(padded_embedding[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomize the list\n",
    "random.shuffle(padded_embedding)\n",
    "\n",
    "training_dataset,test_dataset = train_test_split(padded_embedding, train_size=.80, test_size=.20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dumping the train and test set\n",
    "\n",
    "\n",
    "pickle.dump( training_dataset, open( \"training_data.p\", \"wb\" ) )\n",
    "\n",
    "pickle.dump( test_dataset, open( \"testing_data.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "## In this cell we segrigate inputs and labels for training the model\n",
    "x, y = list(), list()\n",
    "nonecount = list()\n",
    "#print('First padding',a[0])\n",
    "for k,i in enumerate(training_dataset):\n",
    "    if isinstance(i,list):\n",
    "        \n",
    "        seq_x, seq_y = i[:len(i) - 1], i[-1]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    else:\n",
    "        nonecount.append(k)\n",
    "print(len(nonecount))\n",
    "\n",
    "# convert the list to numpy array\n",
    "num_x_train = np.array(x)\n",
    "num_y_train = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "## In this cell we segrigate inputs and labels for testing the model\n",
    "x_test, y_test = list(), list()\n",
    "nonecount1 = list()\n",
    "#print('First padding',a[0])\n",
    "for k,i in enumerate(test_dataset):\n",
    "    if isinstance(i,list):\n",
    "        \n",
    "        seq_x_test, seq_y_test = i[:len(i) - 1], i[-1]\n",
    "        x_test.append(seq_x_test)\n",
    "        y_test.append(seq_y_test)\n",
    "    else:\n",
    "        nonecount.append(k)\n",
    "print(len(nonecount1))\n",
    "\n",
    "# convert the list to numpy array\n",
    "num_x_test = np.array(x_test)\n",
    "num_y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "output_embeddings = []\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 9\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 50\n",
    "x = num_x_train.reshape((num_x_train.shape[0], num_x_train.shape[1], n_features))\n",
    "y = num_y_train\n",
    "#print(x)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(50))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(x, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = num_x_test\n",
    "x_input = x_input.reshape((num_x_test.shape[0], n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "#print(yhat)\n",
    "output_embeddings.append(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate lstm example\n",
    "## with sgd optimizer\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "output_embeddings = []\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 9\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 50\n",
    "x = num_x_train.reshape((num_x_train.shape[0], num_x_train.shape[1], n_features))\n",
    "y = num_y_train\n",
    "#print(x)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(50))\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='mse')\n",
    "# fit model\n",
    "model.fit(x, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = num_x_test\n",
    "x_input = x_input.reshape((num_x_test.shape[0], n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "#print(yhat)\n",
    "output_embeddings.append(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dumping the sample outputs\n",
    "pickle.dump( output_embeddings, open( \"output_embeddings.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dumping the original outputs\n",
    "pickle.dump( num_y_test, open( \"original_output_embeddings.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for efficient memory\n",
    "del padded_embedding\n",
    "del training_dataset,test_dataset\n",
    "del num_x_train\n",
    "del num_y_train\n",
    "del num_x_test\n",
    "\n",
    "del x_test\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the overall embeddings received from Pyke\n",
    "\n",
    "entire_embeddings = pickle.load( open( \"Pyke50_pickle.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151156\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary with only the item embeddings\n",
    "\n",
    "\n",
    "# dictionary of item embeddings\n",
    "item_embedddings = {} \n",
    "for key, val in entire_embeddings.items(): \n",
    "    if key.startswith('http://hm2.com/article#'):\n",
    "        \n",
    "        \n",
    "        item_embedddings.update( {key : val} )\n",
    "          \n",
    "print(len(item_embedddings))\n",
    "pickle.dump( item_embedddings, open( \"item_embedddings.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert output embedings into 1 -D\n",
    "\n",
    "new_output_embeddings = list(itertools.chain.from_iterable(output_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell calculates the cosine similarity among y and y_hats\n",
    "final_cosine = []\n",
    "\n",
    "\n",
    "for yhat in new_output_embeddings:\n",
    "    local_cosine = []\n",
    "    for k,v in item_embedddings.items():\n",
    "\n",
    "        result = 1 - spatial.distance.cosine(yhat, v)\n",
    "        local_cosine.append(result)\n",
    "    final_cosine.append(local_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([14,  8,  7, 11,  9, 10, 12, 13,  6,  1]),\n",
       " array([13,  2,  3,  7, 12,  8,  6,  1, 11,  4]),\n",
       " array([14,  8,  7, 11,  9, 10, 12, 13,  6,  1])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this cell ranks the urls in decending order and lists the top n most frequent elemnts\n",
    "ranks = []\n",
    "for i in final_cosine:\n",
    "    numpy_cosine = np.array(i)\n",
    "    rank_list = numpy_cosine.argsort()[::-1][:50] ## take top 50 prediction for each item\n",
    "    ranks.append(rank_list)\n",
    "\n",
    "#ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell captures y_hat predictions\n",
    "y_hat = []\n",
    "for lists in ranks:\n",
    "    local_key = []\n",
    "    for key in lists:\n",
    "        first_key = list(item_embedddings)[key]\n",
    "        local_key.append(first_key)\n",
    "    y_hat.append(local_key)\n",
    "#y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load original keys of y\n",
    "y = []\n",
    "original_output_embeddings = num_y_test.tolist()\n",
    "#print(len(original_sample_output_embeddings))\n",
    "for i in original_output_embeddings:\n",
    "    for k,v in item_embedddings.items():\n",
    "        if i ==v:\n",
    "            y.append(k)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Ratio is :  0.0\n"
     ]
    }
   ],
   "source": [
    "## This cell is to generate the matrix for final learning\n",
    "## so if original label is present in top n prediction labels , increase the count by 1\n",
    "y_arr = [None]* len(y)\n",
    "y_hat_arr = [None]* len(y_hat)\n",
    "y_arr = y\n",
    "y_hat_arr = y_hat\n",
    "\n",
    "i = 0\n",
    "count = 0\n",
    "while(i<len(y_arr)):\n",
    "    if (y_arr[i] in y_hat_arr[i]):\n",
    "        count +=1\n",
    "    i +=1\n",
    "Ratio = count/len(y_arr)\n",
    "print('Learning Ratio is : ',Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2, 3, 4, 1, 0]), array([2, 3, 4, 1, 0])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_cosine = [[0.9999998221032514, 0.9999998823736738, 0.9999999174768217, 0.999999898314237, 0.9999998957033124], [0.9999998543873563, 0.9999999146544587, 0.99999994975972, 0.9999999305959917, 0.9999999279861516]]\n",
    "ranks = []\n",
    "for i in final_cosine:\n",
    "    numpy_cosine = np.array(i)\n",
    "    rank_list = numpy_cosine.argsort()[::-1][:7] ## take top 50 prediction for each item\n",
    "    ranks.append(rank_list)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
